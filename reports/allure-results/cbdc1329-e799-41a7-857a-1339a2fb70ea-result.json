{"name": "English prompt: Validate AI response accuracy against golden dataset", "status": "failed", "statusDetails": {"message": "AssertionError: gdrfa_hours_en failed; score=0.7667510509490967, facts=1/2\nassert False", "trace": "logged_in_page = <Page url='https://govgpt.sandbox.dge.gov.ae/c/ebfcd5f4-8b55-448e-afe7-7dc5c30c1cde'>\nprompt_case = {'golden': 'GDRFA Dubai operates Sunday to Thursday during typical government hours; check the official website for the latest timings.', 'id': 'gdrfa_hours_en', 'lang': 'en', 'must_contain': ['GDRFA', 'timings'], ...}\n\n    @allure.epic(\"English accuracy vs golden responses\")\n    @allure.feature(\"Accuracy vs Golden Responses\")\n    @allure.story(\"System should generate factual, relevant, and well-formatted answers\")\n    @allure.title(\"English prompt: Validate AI response accuracy against golden dataset\")\n    @allure.description(\"\"\"\n    Objective:\n    To verify that for known English prompts, the AI gives accurate and relevant answers\n    that match expected golden responses or contain required facts.\n    \"\"\")\n    @pytest.mark.parametrize(\"prompt_case\", [\n        pytest.param(p, id=p.get(\"id\", \"case\"))\n        for p in json.load(open(\"data/test-data.json\", encoding=\"utf-8\")).get(\"prompts\", [])\n        if p.get(\"lang\") == \"en\"\n    ])\n    def test_semantic_en_accuracy(logged_in_page, prompt_case):\n        page = logged_in_page\n        p = prompt_case\n        prompt = p.get(\"user\") or p.get(\"prompt\", \"\")\n        golden = (p.get(\"golden\") or \"\").strip()\n        ans = _send_and_get_answer(page, prompt)\n        facts = [f.lower() for f in p.get(\"must_contain\", [])]\n    \n        score = sim_en(ans, golden) if golden else None\n        hits = sum(1 for f in facts if f in _norm(ans)) if facts else 0\n        needed = max(1, min(2, len(facts))) if facts else 0\n    \n        if golden:\n            thr = float(p.get(\"threshold\", 0.70))\n            ok = score is not None and score >= thr\n        else:\n            ok = (len(ans.strip()) > 50) and (hits >= needed)\n    \n        # Reporting to Allure\n        allure.attach(prompt, f\"prompt::{p.get('id','case')}\", allure.attachment_type.TEXT)\n        allure.attach(ans, f\"app_answer::{p.get('id','case')}\", allure.attachment_type.TEXT)\n        if golden:\n            allure.attach(golden, f\"golden::{p.get('id','case')}\", allure.attachment_type.TEXT)\n            allure.attach(f\"{score:.3f}\", f\"sim_en::{p.get('id','case')}\", allure.attachment_type.TEXT)\n    \n        if not ok:\n            take_screenshot(page, f\"fail_{p.get('id','case')}\")\n            attach_dom(page, f\"dom_fail_{p.get('id','case')}\")\n    \n>       assert ok, f\"{p.get('id','case')} failed; score={score}, facts={hits}/{len(facts)}\"\nE       AssertionError: gdrfa_hours_en failed; score=0.7667510509490967, facts=1/2\nE       assert False\n\ntests\\genai\\test_semantic.py:75: AssertionError"}, "description": "\nObjective:\nTo verify that for known English prompts, the AI gives accurate and relevant answers\nthat match expected golden responses or contain required facts.\n", "attachments": [{"name": "prompt::gdrfa_hours_en", "source": "44f490c1-5414-498d-af22-a0d2b7f9e76b-attachment.txt", "type": "text/plain"}, {"name": "app_answer::gdrfa_hours_en", "source": "3679ecee-a675-41ca-a08f-3201df7b4f07-attachment.txt", "type": "text/plain"}, {"name": "golden::gdrfa_hours_en", "source": "be08a2a2-614f-4f34-8310-a3772fc72cbc-attachment.txt", "type": "text/plain"}, {"name": "sim_en::gdrfa_hours_en", "source": "722c941f-afee-4d6c-8a91-86a705583b69-attachment.txt", "type": "text/plain"}, {"name": "fail_gdrfa_hours_en", "source": "967070a4-c3c4-42eb-8d50-a33973259d27-attachment.png", "type": "image/png"}, {"name": "dom_fail_gdrfa_hours_en", "source": "8b5b75e9-1f9a-4a59-b936-28657299c1c0-attachment.html", "type": "text/html"}, {"name": "FAILED_test_semantic_en_accuracy[gdrfa_hours_en]", "source": "63c0b59b-c908-4886-b98f-480bf18434ba-attachment.png", "type": "image/png"}, {"name": "DOM_test_semantic_en_accuracy[gdrfa_hours_en]", "source": "fc5e4739-0e1b-475a-9975-81e4428a6a8f-attachment.html", "type": "text/html"}], "parameters": [{"name": "prompt_case", "value": "{'id': 'gdrfa_hours_en', 'lang': 'en', 'user': 'What are the working hours of GDRFA Dubai?', 'golden': 'GDRFA Dubai operates Sunday to Thursday during typical government hours; check the official website for the latest timings.', 'threshold': 0.8, 'must_contain': ['GDRFA', 'timings']}"}], "start": 1755619344415, "stop": 1755619364093, "uuid": "17828f68-cc5c-47f3-aebc-c6aad28c9c2e", "historyId": "d11cbcf912d960d038be39e543829f22", "testCaseId": "da016600cef80289dc31a7b465ec066f", "fullName": "tests.genai.test_semantic#test_semantic_en_accuracy", "labels": [{"name": "story", "value": "System should generate factual, relevant, and well-formatted answers"}, {"name": "feature", "value": "Accuracy vs Golden Responses"}, {"name": "epic", "value": "English accuracy vs golden responses"}, {"name": "parentSuite", "value": "tests.genai"}, {"name": "suite", "value": "test_semantic"}, {"name": "host", "value": "LAPTOP-ITSGITUD"}, {"name": "thread", "value": "8660-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.genai.test_semantic"}]}