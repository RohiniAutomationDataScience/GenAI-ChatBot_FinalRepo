{"name": "English prompt: Validate AI response accuracy against golden dataset", "status": "failed", "statusDetails": {"message": "AssertionError: eid_en failed; score=0.7535176277160645, facts=3/3\nassert False", "trace": "logged_in_page = <Page url='https://govgpt.sandbox.dge.gov.ae/c/555d2b32-b119-47a8-a7a8-669c1f90b3ea'>\nprompt_case = {'golden': 'Renew your Emirates ID through the ICP Smart Services portal. Submit biometrics if required and pay the renewal fee.', 'id': 'eid_en', 'lang': 'en', 'must_contain': ['ICP', 'Emirates ID', 'biometric'], ...}\n\n    @allure.epic(\"English accuracy vs golden responses\")\n    @allure.feature(\"Accuracy vs Golden Responses\")\n    @allure.story(\"System should generate factual, relevant, and well-formatted answers\")\n    @allure.title(\"English prompt: Validate AI response accuracy against golden dataset\")\n    @allure.description(\"\"\"\n    Objective:\n    To verify that for known English prompts, the AI gives accurate and relevant answers\n    that match expected golden responses or contain required facts.\n    \"\"\")\n    @pytest.mark.parametrize(\"prompt_case\", [\n        pytest.param(p, id=p.get(\"id\", \"case\"))\n        for p in json.load(open(\"data/test-data.json\", encoding=\"utf-8\")).get(\"prompts\", [])\n        if p.get(\"lang\") == \"en\"\n    ])\n    def test_semantic_en_accuracy(logged_in_page, prompt_case):\n        page = logged_in_page\n        p = prompt_case\n        prompt = p.get(\"user\") or p.get(\"prompt\", \"\")\n        golden = (p.get(\"golden\") or \"\").strip()\n        ans = _send_and_get_answer(page, prompt)\n        facts = [f.lower() for f in p.get(\"must_contain\", [])]\n    \n        score = sim_en(ans, golden) if golden else None\n        hits = sum(1 for f in facts if f in _norm(ans)) if facts else 0\n        needed = max(1, min(2, len(facts))) if facts else 0\n    \n        if golden:\n            thr = float(p.get(\"threshold\", 0.70))\n            ok = score is not None and score >= thr\n        else:\n            ok = (len(ans.strip()) > 50) and (hits >= needed)\n    \n        # Reporting to Allure\n        allure.attach(prompt, f\"prompt::{p.get('id','case')}\", allure.attachment_type.TEXT)\n        allure.attach(ans, f\"app_answer::{p.get('id','case')}\", allure.attachment_type.TEXT)\n        if golden:\n            allure.attach(golden, f\"golden::{p.get('id','case')}\", allure.attachment_type.TEXT)\n            allure.attach(f\"{score:.3f}\", f\"sim_en::{p.get('id','case')}\", allure.attachment_type.TEXT)\n    \n        if not ok:\n            take_screenshot(page, f\"fail_{p.get('id','case')}\")\n            attach_dom(page, f\"dom_fail_{p.get('id','case')}\")\n    \n>       assert ok, f\"{p.get('id','case')} failed; score={score}, facts={hits}/{len(facts)}\"\nE       AssertionError: eid_en failed; score=0.7535176277160645, facts=3/3\nE       assert False\n\ntests\\genai\\test_semantic.py:75: AssertionError"}, "description": "\nObjective:\nTo verify that for known English prompts, the AI gives accurate and relevant answers\nthat match expected golden responses or contain required facts.\n", "attachments": [{"name": "prompt::eid_en", "source": "623536a0-aaa5-4132-83f6-1465b3a9d4d1-attachment.txt", "type": "text/plain"}, {"name": "app_answer::eid_en", "source": "5b838fb6-76a9-4402-955b-f0a33be0d1b8-attachment.txt", "type": "text/plain"}, {"name": "golden::eid_en", "source": "68f5c351-bd46-4ceb-b3f0-7d062a1e9093-attachment.txt", "type": "text/plain"}, {"name": "sim_en::eid_en", "source": "a59361c6-f0f8-4e59-9be6-47ad85fe6da1-attachment.txt", "type": "text/plain"}, {"name": "fail_eid_en", "source": "37c10ebc-16e9-4bc2-ac12-e3440e307473-attachment.png", "type": "image/png"}, {"name": "dom_fail_eid_en", "source": "b31de4d6-6cac-4e6c-be3f-d933c0d2d1d1-attachment.html", "type": "text/html"}, {"name": "FAILED_test_semantic_en_accuracy[eid_en]", "source": "388bef25-aa22-48e6-832a-4cff57621676-attachment.png", "type": "image/png"}, {"name": "DOM_test_semantic_en_accuracy[eid_en]", "source": "fb4f7278-cce5-4deb-af70-2bd961cb9189-attachment.html", "type": "text/html"}], "parameters": [{"name": "prompt_case", "value": "{'id': 'eid_en', 'lang': 'en', 'user': 'How do I renew my Emirates ID?', 'golden': 'Renew your Emirates ID through the ICP Smart Services portal. Submit biometrics if required and pay the renewal fee.', 'threshold': 0.8, 'must_contain': ['ICP', 'Emirates ID', 'biometric']}"}], "start": 1755619283100, "stop": 1755619329702, "uuid": "dea4f422-b1f3-45bf-bb2b-7be16c5ec46b", "historyId": "e31062f72ee184f910cfb99dbd9b83b0", "testCaseId": "da016600cef80289dc31a7b465ec066f", "fullName": "tests.genai.test_semantic#test_semantic_en_accuracy", "labels": [{"name": "story", "value": "System should generate factual, relevant, and well-formatted answers"}, {"name": "feature", "value": "Accuracy vs Golden Responses"}, {"name": "epic", "value": "English accuracy vs golden responses"}, {"name": "parentSuite", "value": "tests.genai"}, {"name": "suite", "value": "test_semantic"}, {"name": "host", "value": "LAPTOP-ITSGITUD"}, {"name": "thread", "value": "8660-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.genai.test_semantic"}]}