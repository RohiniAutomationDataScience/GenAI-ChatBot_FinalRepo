{"name": "English prompt: Validate AI response accuracy against golden dataset", "status": "passed", "description": "\nObjective:\nTo verify that for known English prompts, the AI gives accurate and relevant answers\nthat match expected golden responses or contain required facts.\n", "attachments": [{"name": "prompt::fallback_test_en", "source": "99232a02-8460-47cf-b1d9-b9e1ba1ff60a-attachment.txt", "type": "text/plain"}, {"name": "app_answer::fallback_test_en", "source": "eaa86396-6037-4f12-a04a-b83b81e9a782-attachment.txt", "type": "text/plain"}, {"name": "golden::fallback_test_en", "source": "3a00e2de-f84a-491d-8481-d86baffd0a54-attachment.txt", "type": "text/plain"}, {"name": "sim_en::fallback_test_en", "source": "657ea74a-88fc-4423-b5b0-fb4b8b0c6e9e-attachment.txt", "type": "text/plain"}], "parameters": [{"name": "prompt_case", "value": "{'id': 'fallback_test_en', 'lang': 'en', 'user': '!@#$%^&*() gibberish input 123', 'golden': \"I'm sorry, I'm not able to assist with your request\", 'threshold': 0.8, 'must_contain': ['sorry', 'not able to assist', 'request']}"}], "start": 1755619371246, "stop": 1755619375857, "uuid": "a4815975-6a4d-4fd6-9aac-523b2a5786bf", "historyId": "1a5603cf76632ce4d1bac36b3d6eee87", "testCaseId": "da016600cef80289dc31a7b465ec066f", "fullName": "tests.genai.test_semantic#test_semantic_en_accuracy", "labels": [{"name": "story", "value": "System should generate factual, relevant, and well-formatted answers"}, {"name": "feature", "value": "Accuracy vs Golden Responses"}, {"name": "epic", "value": "English accuracy vs golden responses"}, {"name": "parentSuite", "value": "tests.genai"}, {"name": "suite", "value": "test_semantic"}, {"name": "host", "value": "LAPTOP-ITSGITUD"}, {"name": "thread", "value": "8660-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.genai.test_semantic"}]}