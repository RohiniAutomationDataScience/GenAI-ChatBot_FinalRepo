{"name": "Validate AI response accuracy and semantics - fallback_test_ar", "status": "failed", "statusDetails": {"message": "AssertionError: [fallback_test_ar] similarity 0.70 facts 0/3\nassert False", "trace": "logged_in_page = <Page url='https://govgpt.sandbox.dge.gov.ae/c/0cb3abf6-d6ee-4158-ae66-30db72fa6bfa'>\ncase = {'golden': 'عذرًا، لم أفهم ذلك. هل يمكنك إعادة الصياغة أو توضيح ما تعنيه؟', 'id': 'fallback_test_ar', 'lang': 'ar', 'must_contain': ['عذرًا', 'إعادة الصياغة', 'توضيح'], ...}\n\n    @allure.epic(\"Response Accuracy and Relevance\")\n    @allure.feature(\"Response Accuracy and Relevance\")\n    @allure.story(\"Validate AI responses against golden answers and fact checks\")\n    @allure.title(\"Validate chatbot response similarity and fact coverage for all prompts - {case[id]}\")\n    @allure.description(\"\"\"\n    Objective:\n    To verify that for all prompts (English and Arabic), the AI response is similar to the golden response\n    when available, contains required factual information, and meets quality thresholds.\n    \"\"\")\n    #@pytest.mark.parametrize(\"case\", DATA.get(\"prompts\", []))\n    @pytest.mark.parametrize(\"case\", DATA.get(\"prompts\", []), ids=lambda c: c.get(\"id\", \"no-id\"))\n    def test_app_vs_golden_similarity(logged_in_page, case):\n        allure.dynamic.title(f\"Validate AI response accuracy and semantics - {case.get('id', 'no-id')}\")\n        page   = logged_in_page\n        user_q = case.get(\"user\") or case.get(\"prompt\",\"\")\n        golden = (case.get(\"golden\") or \"\").strip()  # may be empty in future\n        lang   = case.get(\"lang\",\"en\")\n        facts  = [f.lower() for f in case.get(\"must_contain\", [])]\n        base_thr = float(case.get(\"threshold\", 0.85 if lang==\"en\" else case.get(\"xl_threshold\", 0.80)))\n    \n        # Ask & capture\n        app_ans = _send_and_get_answer(page, user_q)\n    \n        # Similarity (if golden present)\n        score = None\n        if golden:\n            score = sim_xl(app_ans, golden) if lang==\"ar\" else sim_en(app_ans, golden)\n    \n        # Fact coverage\n        app_norm = _norm(app_ans)\n        hits = sum(1 for f in facts if f in app_norm) if facts else 0\n        needed = max(1, min(2, len(facts))) if facts else 0\n    \n        # Length-aware relax (only if golden exists)\n        if golden:\n            len_ratio = max(1.0, len(app_ans)/max(1,len(golden)))\n            relax = 0.10 if len_ratio >= 3.0 else (0.05 if len_ratio >= 1.8 else 0.0)\n            eff_thr = max(0.70, base_thr - relax)\n            ok = (score >= base_thr) or ((score >= eff_thr) and (hits >= needed))\n        else:\n            # No golden: require substantive answer + facts hit (when provided)\n            ok = (len(app_ans.strip()) > 50) and (hits >= needed)\n    \n        # Allure attachments\n        allure.attach(user_q, \"prompt\", allure.attachment_type.TEXT)\n        allure.attach(app_ans, \"app_answer\", allure.attachment_type.TEXT)\n        if golden:\n            allure.attach(golden, \"golden_answer\", allure.attachment_type.TEXT)\n            allure.attach(f\"{score:.3f}\", \"similarity\", allure.attachment_type.TEXT)\n            allure.attach(\n                f\"url={page.url}\\n\"\n                f\"facts_hit={hits}/{len(facts)}\\n\"\n                f\"base_thr={base_thr:.2f}\\n\"\n                f\"eff_thr={eff_thr:.2f}\\n\",\n                \"diagnostics\",\n                allure.attachment_type.TEXT\n            )\n        else:\n            allure.attach(\n                f\"url={page.url}\\n\"\n                f\"facts_hit={hits}/{len(facts)}\\n\"\n                f\"no_golden=True\\n\",\n                \"diagnostics\",\n                allure.attachment_type.TEXT\n            )\n    \n        if not ok:\n            take_screenshot(page, f\"fail_{case.get('id','case')}_fullpage\")\n            try:\n                resp_el = page.locator(\"(//div[@id='response-content-container'])[last()]\")\n                if resp_el.count():\n                    allure.attach(resp_el.screenshot(), f\"fail_{case.get('id','case')}_response\", allure.attachment_type.PNG)\n            except Exception:\n                pass\n            attach_dom(page, f\"dom_fail_{case.get('id','case')}\")\n    \n>       assert ok, (\n            f\"[{case.get('id','case')}] \"\n            + (f\"similarity {score:.2f} \" if score is not None else \"no_golden \")\n            + f\"facts {hits}/{len(facts)}\"\n        )\nE       AssertionError: [fallback_test_ar] similarity 0.70 facts 0/3\nE       assert False\n\ntests\\genai\\test_against_golden.py:97: AssertionError"}, "description": "\nObjective:\nTo verify that for all prompts (English and Arabic), the AI response is similar to the golden response\nwhen available, contains required factual information, and meets quality thresholds.\n", "attachments": [{"name": "prompt", "source": "1752908b-0c16-4fb9-b9ec-66a2de3727ce-attachment.txt", "type": "text/plain"}, {"name": "app_answer", "source": "bb95e581-e08f-4ffe-b705-6905359acb41-attachment.txt", "type": "text/plain"}, {"name": "golden_answer", "source": "b16370a3-884d-4c0b-b5ba-727c2cd888ca-attachment.txt", "type": "text/plain"}, {"name": "similarity", "source": "e900b284-243f-4247-b258-e9eb22ac023d-attachment.txt", "type": "text/plain"}, {"name": "diagnostics", "source": "6d58a339-c818-41a1-b11d-7189eaf63370-attachment.txt", "type": "text/plain"}, {"name": "fail_fallback_test_ar_fullpage", "source": "8367411b-51cc-40a6-9784-721d1cbf5d12-attachment.png", "type": "image/png"}, {"name": "fail_fallback_test_ar_response", "source": "fe462661-7ca9-439d-8492-a708c9770301-attachment.png", "type": "image/png"}, {"name": "dom_fail_fallback_test_ar", "source": "02c83c42-09c0-4a1a-b376-fb4d7e5cb406-attachment.html", "type": "text/html"}, {"name": "FAILED_test_app_vs_golden_similarity[fallback_test_ar]", "source": "855442c6-2988-44df-895a-e2b72f4f09d2-attachment.png", "type": "image/png"}, {"name": "DOM_test_app_vs_golden_similarity[fallback_test_ar]", "source": "c6087580-e1b9-413a-a43a-afba04c84006-attachment.html", "type": "text/html"}], "parameters": [{"name": "case", "value": "{'id': 'fallback_test_ar', 'lang': 'ar', 'user': '!@#$٪^&*() معلومات غير مفهومة 123', 'golden': 'عذرًا، لم أفهم ذلك. هل يمكنك إعادة الصياغة أو توضيح ما تعنيه؟', 'xl_threshold': 0.8, 'must_contain': ['عذرًا', 'إعادة الصياغة', 'توضيح']}"}], "start": 1755618846101, "stop": 1755618851869, "uuid": "d6d4d24f-3297-4015-94ae-af3c678bea06", "historyId": "0d0b1de234b75fe8093f8f5cdfe94430", "testCaseId": "4351bef7262dccc0f46b030dd9a53ecd", "fullName": "tests.genai.test_against_golden#test_app_vs_golden_similarity", "labels": [{"name": "epic", "value": "Response Accuracy and Relevance"}, {"name": "feature", "value": "Response Accuracy and Relevance"}, {"name": "story", "value": "Validate AI responses against golden answers and fact checks"}, {"name": "parentSuite", "value": "tests.genai"}, {"name": "suite", "value": "test_against_golden"}, {"name": "host", "value": "LAPTOP-ITSGITUD"}, {"name": "thread", "value": "8660-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "tests.genai.test_against_golden"}]}